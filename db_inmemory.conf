include  required(classpath("application"))

call-caching {
    enabled = true
    invalidate-bad-cache-results = true
}

backend {
  default = "PBSPRO"
  providers {
    PBSPRO {
      actor-factory = "cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"
      config {
        
        filesystems {
            local {
                localization: [
                    # soft link does not work with containers. Hard links won't work across file systems
                    "hard-link","cached-copy","copy"
                ]

                caching {
                    # When copying a cached result, what type of file duplication should occur.
                    # Attempted in the order listed below:
                    duplication-strategy: [
                        "hard-link","cached-copy","copy"
                    ]
                    #tries to create a fingerprint for each file by taking its last modified time (milliseconds since epoch in hexadecimal) + size (bytes in hexadecimal) + the xxh64 sum of the first 10 MB of the file
                    hashing-strategy: "fingerprint"
                    # default size 10485760 or (10MB)
                    fingerprint-size: 10485760
                }
            }
        }

        concurrent-job-limit = 30
        
        runtime-attributes = """
        String? docker
        String  walltime = "240:00:00"
        String  queue = "cgsd"
        Int     cpu = 4
        Int     memory_gb = 4
        """
        
        submit = """
        qsub \
        -k oe \
        -l walltime=${walltime} \
        -q ${queue} \
        -l ncpus=${cpu} \
        -l mem=${memory_gb}gb \
        -o ${out} \
        -e ${err} \
        ${script}
        """
        
        submit-docker = """
               
        module load singularity

        # if you prefer to have singularity cache in $HOME, delete the following two lines
        
        mkdir -p /groups/cgsd/$USER/.singularity
        # export SINGULARITY_CACHEDIR=/groups/cgsd/$USER/.singularity
        
        # Singularity cachedir check if it exists, if not create one        
        
        if [ -z $SINGULARITY_CACHEDIR ];
          then CACHE_DIR=$HOME/.singularity/cache
          else CACHE_DIR=$SINGULARITY_CACHEDIR
        fi

        # make sure cache dir exists, so we can create a lock file with flock command
        
        mkdir -p $CACHE_DIR
        LOCK_FILE=$CACHE_DIR/singularity_pull_flock
        
        # Create an exclusive filelock with flock.

        flock --exclusive --timeout 900 $LOCK_FILE \
        singularity exec  -C docker://${docker} \
        echo "successfully pulled ${docker}!"

        # Submit the script 
        qsub \
        -W block=true \
        -k oed \
        -v SINGULARITY_CACHEDIR \
        -N ${job_name} \
        -l walltime=${walltime} \
        -q ${queue} \
        -l ncpus=${cpu} \
        -l mem=${memory_gb}gb \
        -o ${cwd}/execution/stdout \
        -e ${cwd}/execution/stderr \
        -- /software/singularity/3.6.3/bin/singularity exec -C --bind ${cwd}:${docker_cwd} docker://${docker} ${job_shell} ${docker_script}
        """
        
        root = "/groups/cgsd/alexandre/cromwell-executions"
        job-id-regex = "(\\d+)"
        kill = "qdel ${job_id}"
        check-alive = "qstat ${job_id}"
      }
    }
  }
}

database {
  profile = "slick.jdbc.HsqldbProfile$"
  db {
    driver = "org.hsqldb.jdbcDriver"
    url = """
    jdbc:hsqldb:file:cromwell-executions/cromwell-db/cromwell-db;
    shutdown=false;
    hsqldb.default_table_type=cached;hsqldb.tx=mvcc;
    hsqldb.result_max_memory_rows=10000;
    hsqldb.large_data=true;
    hsqldb.applog=1;
    hsqldb.lob_compressed=true;
    hsqldb.script_format=3
    """
    connectionTimeout = 120000
    numThreads = 1
   }
}
